### 1. Основные свойства численных методов.
#### Устойчивость
Решение задачи $y^*$ называется устойчивым по исходным данным $x^*$, если оно зависит от исходных данных непрерывным образом. Это означает, что малому изменению исходных данных соответствует малое изменение решения. Алгоритм считается устойчивым, если он обеспечивает нахождение существующего и единственного решения при различных исходных данных.
#### Сходимость
Численное решение задачи должно стремиться к точному решению задачи
Алгоритм сходится, если последовательность приближений $x_0, x_1, ..., x_{n} \to x^*, n\to \infty , \lim_{ n \to \infty }x_{n}=x^*$
// $x^*$ - это истинное/точное значение, к которому мы стремимся
#### Корректность
Численные методы применяются к корректно поставленным задачам
Задача называется поставленной корректно, если выполняются следующие условия:
1) Решение задачи существует и единственно при любых допустимых исходных данных
2) Решение устойчиво по отношению к малым изменениям исходных данных
### 2. Решение систем линейных уравнений. Метод Гаусса.
https://www.youtube.com/watch?v=M4Izk1gkdfY
#### Прямой ход:
Последовательное исключение неизвестных из уравнений системы. Сначала с помощью первого уравнения исключается $x_{1}$ из всех последующих уравнений системы. Затем с помощью второго уравнения исключаются $x_{2}$ из третьего и всех последующих уравнений и т.д.
Этот процесс продолжается до тех пор, пока в левой части последнего (n-го) уравнения не останется лишь один член с неизвестным $x_{n}$, то есть матрица системы будет приведена к треугольному виде
#### Обратный ход:
Последовательное вычисление искомых неизвестных: решая последнее уравнение, находим неизвестное $x_{n}$. Далее, из предыдущего уравнения вычисляем $x_{n-1}$ и так далее. Последним найдем $x_1$ из первого уравнения.

Метод имеет много различных вычислительных схем
#### Основное требование
$\det A\neq 0$


Если кратко
Надо представить СЛУ в виде матрицы
1 2 3 | 4
5 6 7 | 8
9 10 11 | 12
И надо получить треугольную матрицу (в данном случае 5, 9 и 10 должны стать нулями) обычной перестановкой и складыванием строк друг с другом
После этого получаем опять систему (числа условные)
$13*x + 14*y +15*z = 16$
$17*y + 18 * z = 19$
$20 * z = 21$
И далее очевидно получаем z, из этого y, и x
### 3. Метод Гаусса с выбором главного элемента.
https://studfile.net/preview/4426799/page:9/ - вроде здесь норм объяснение

Если вкратце: по сути это тот же Гаусс, но мы не тупо делим на левый ненулевой коэффициент все не-верхние строки, а сортируем по максимальному
Можно сортировать по столбцам или строкам
### 4. Метод Гаусса-Зейделя.
По сути как в простой итерации
Переносим x1, x2, x3 в левую часть
Но если в методе простой итерации при подстановке иксов мы вставляем значения из прошлой итерации, то в Гаусса-Зейделя мы используем вот такую схему
$$
\begin{align}
x_{1}^1 \to x^0_{2} \quad x^0_{3} \quad x^0_{4} \\
x_{2}^1 \to x^1_{1} \quad x^0_{3} \quad x^0_{4} \\
x_{3}^1 \to x^1_{1} \quad x^1_{2} \quad x^0_{4} \\
x_{4}^1 \to x^1_{1} \quad x^1_{2} \quad x^1_{3} \\
\end{align}

$$

### 5. Метод простой итерации.
Переносим диагональные элементы из каждой строчки влево, все остальное вправо
Для первого приближения делим b на a
https://www.youtube.com/watch?v=4s9GxRY_DNM

Рассмотрим СЛАУ с невырожденной матрицей ($\det A\neq0$)

$$
\begin{cases}
a_{11}x_{1}+a_{12}x_{2}+\dots+a_{1n}x_{n}=b_{1} \\
a_{21}x_{1}+a_{22}x_{2}+\dots+a_{2n}x_{n} = b_{2} \\
........ \\
a_{n1}x_{1}+a_{n2}x_{2}+\dots+a_{nn}x_{n}=b_{n}
\end{cases}
$$
Приведем систему уравнений к следующему виду, выразив неизвестные $x_{1}, x_{2}, \dots, x_{n}$ соответственно из первого, второго и т.д. уравнений первой системы:
$$
\begin{cases}
x_{1}=-\frac{a_{12}}{a_{11}}x_{2}-\frac{a_{13}}{a_{11}}x_{3}-\dots-\frac{a_{1n}}{a_{11}}x_{n}+b_{1}a_{11} \\
x_{2}=-\frac{a_{21}}{a_{22}}x_{1}-\frac{a_{23}}{a_{22}}x_{3}-\dots-\frac{a_{2n}}{a_{22}}x_{n}+\frac{b_{2}}{a_{22}} \\
x_{n}=-\frac{a_{n_{1}}}{a_{nn}}x_{1}-\frac{a_{n_{2}}}{a_{nn}}x_{2}-\dots-\frac{a_{n-1n-1}}{a_{nn}}x_{n-1}+\frac{b_{n}}{a_{nn}}
\end{cases}
$$
Обозначим
$$
c_{ij}=\begin{cases}
0, \text{ при } i=j \\
-\frac{a_{ij}}{a_{ii}}, \text{ при }i\neq j
\end{cases}
$$
$$
d_{i}=\frac{b_{i}}{a_{ii}};i=1, 2,\dots ,n
$$
Тогда получим:
$$
\begin{cases}
x_{1}=c_{11}x_{1}+c_{12}x_{2}+\dots+c_{1n}x_{n}+d_{1} \\
x_{2}=c_{21}x_{1}+c_{22}x_{2}+\dots+c_{2n}x_{n}+d_{2} \\
\dots \\
x_{n}=c_{n_{1}}x_{1}+c_{n_{2n}}x_{2}+\dots c_{nn}x_{n}+d_{n}
\end{cases}
$$
Или в векторно-матричном виде: $x=Cx+D$, где x - вектор неизвестных, $C$ - матрица коэффициентов преобразованной системы размерности  $n*n, \text{ } D$ - вектор правых частей преобразованной системы

Систему можно представить в сокращенном виде:
$$
x_{i}=\sum^n_{j=1}c_{ij}x_{j}+d_{i},\quad i=1, 2, \dots ,n
$$
Рабочая формула:
$$
x_{i}^{k+1}=\frac{b_{i}}{a_{ii}}-\sum^n_{j=1;j\neq i}\frac{a_{ij}}{a_{ii}}x^k_{j}; \quad i=1, 2, \dots, n; \quad k \text{ - номер итерации}
$$
За начальное приближение выбирают вектор свободных членов: $x^{(0)}=D$ или нулевой вектор $x^{(0)}=0$

Достаточным условием сходимости итерационного метода к решению системы при любом начальном векторе $x_i^{(0)}$ является требование к норме матрицы $C$:
$$||C||<1$$
$$
||C||=max_{1\leq i\leq n}\sum^n_{j=1}|c_{ij}|<1
$$
$$
||C||=max_{1\leq j \leq n}\sum^n_{i=1}|c_{ij}|<1
$$
Условие сходимости $||C||<1$ в этом методе равносильно условию диагонального преобладания

#### +:
- Универсальный и простой для реализации на ЭВМ
#### -:
- Является трудоемким
- Обладает медленной сходимостью сходимости
### 6. Условия сходимости итерационных методов решения СЛАУ.
Теорема
Достаточным условием сходимости итерационного процесса к решению системы при любом начальном векторе $x_{i}^{(0)}$ является выполнение условия преобладания диагональных элементов или доминирование диагонали: $|a_{ii}|\geq \sum_{j\neq i}|a_{ij}|, i = 1, 2, \dots n$
При этом хотя бы для одного уравнения неравенство должно выполняться строго. Эти условия являются достаточными для сходимости метода, но они не являются необходимыми, т.е. для некоторых систем итерации сходятся и при нарушении этого условия.
### 7. Методы решения нелинейных уравнений. Метод касательных (Ньютона).
Идея метода:
$y=f(x)$ на отрезке $[a, b]$ заменяется касательной и в качестве приближенного значения корня принимается точка пересечения касательной с осью абсцисс.
Пусть $x_{o}\in[a, b]$ - начальное приближение. Запишем уравнение касательной к графику функции $y=f(x)$ в этой точке: $y=f(x_{0})+f'(x_{0})(x-x_{0})$
Найдем пересечение касательной с осью $x$:
$x_{1}=x_{0}-\frac{{f(x_{0})}}{f'(x_{0})}$

Рабочая формула метода: 
$x_{i}=x_{i-1}-\frac{{f(x_{i-1})}}{f'(x_{i-1})}$
Критерий окончания итерационного процесса:
$|x_{n}-x_{n-1}|\leq \epsilon$ или $|\frac{f(x_{n})}{f'(x_{n})}|\leq \epsilon$ или $f(x_{n})\leq\epsilon$
Приближенное значение корня: $x^*=x_{n}$

Сходимость зависит от того, насколько близко корню выбрано начальное приближение. Тогда скорость сходимости велика.
Метод Ньютона эффективен, если выполняются условия сходимости:
- Производные $f'(x)$ и $f''(x)$ сохраняют знак на отрезке $[a;b]$
- Производная $f'(x)\neq{0}$

Метод обеспечивает быструю сходимость, если выполняется: $f(x_{0})*f''(x_{0})>0$
То есть лучше выбирать тот конец интервала, у которого знаки функций второй и первой производной совпадают
### 8. Методы решения нелинейных уравнений. Метод деления отрезка пополам.
##### Идея:
1. Начальный интервал изоляции корня делим пополам, получаем начальное приближение к корню: $x_{0}= \frac{a_{0}+b_{0}}{2}$
2. Вычисляем $f(x_{0})$
3. В качестве нового интервала выбираем ту половину отрезка, на концах которого функция имеет разные знаки: $[a_{0}, x_{0}]$ либо $[b_{0}, x_{0}]$. Другую половину отрезка $[a_{0}, b_{0}]$ (на которой функция знак не меняет) отбрасываем.
4. Новый интервал вновь делим пополам, получаем очередное приближение к корню: $x_{1}=\frac{a_{1}+b_{1}}{2}$ и т.д.

##### Рабочая формула метода: 
$$
x_{i}=\frac{a_{i}+b_{i}}{2}
$$
Приближенное значение корня: $x^*=\frac{a_{n}+b_{n}}{2}$ или $x^*=a_{n}$ или $x^*=b_{n}$

##### Сходимость итерационного процесса фиксируется следующими способами:
1. Сходимость по аргументу: $x_{n}-x_{n-1}\leq\epsilon$
2. Сходимость по функции: $|f(x_{n})|\leq \epsilon$
3. КОНКРЕТНО ДЛЯ ЭТОГО МЕТОДА еще вот так можно: $|a_{n}-b_{n}|<\epsilon$
### 9. Методы решения нелинейных уравнений. Метод простой итерации.

###### 1. Задача
Хотим решить нелинейное уравнение:
$$f(x)=0.$$
###### 2. Идея метода
Вместо того чтобы работать с $f(x)=0$, мы переписываем уравнение в **эквивалентной форме**: $x = \varphi(x)$.
Например:
- Если $f(x)=x^2-2=0$, можно переписать как $x=\sqrt{2}$, т.е. $\varphi(x)=\sqrt{2}$.
- Или как $x=\tfrac{2}{x}$, т.е. $x\varphi(x)=\tfrac{2}{x}.$  
    Разные переписывания дадут разные итерационные процессы.
###### 3. Итерационный процесс
Стартуем с какого-то приближения $x_0$.  
Затем строим последовательность:
$x_{k+1} = \varphi(x_k).$
То есть просто подставляем текущее значение в правую часть и получаем новое.
###### 4. Условие сходимости
Чтобы это работало, нужно, чтобы $\varphi(x)$ была «сжимающей».  
Формально: $|\varphi'(x)| < 1$ на интервале, где ищем корень}.
Это условие гарантирует, что приближения будут сближаться к истинному решению.
###### 5. Критерий остановки
На практике итерации прекращают, когда изменения стали достаточно малы:
$|x_{k+1}-x_k| \le \varepsilon.$
###### 6. Особенность
Метод простой итерации — **очень простой**, но сходится **медленно** (обычно линейно).
### 10. Методы решения нелинейных уравнений. Метод хорд.
##### Идея:
Функция $y=f(x)$ на отрезке $[a, b]$ заменяется хордой и в качестве приближенного значения корня принимается точка пересечения хорды с осью абсцисс
Уравнение хорды, проходящей через точки $A(a, f(a))$ и $B(b, f(b))$: 
$$
\frac{y-f(a)}{f(b)-f(a)}=\frac{x-a}{b-a}
$$
Точка пересечения хорды с осью абсцисс $y=0$: $x=a-\frac{b-a}{f(b)-f(a)}f(a)$
##### Алгоритм:
1. Находим интервал изоляции корня $[a_{0}, b_{0}]$
2. Вычисляем $x_{0}:$ $x_{0}=a_{0}- \frac{b_{0}-a_{0}}{f(b_{0})-f(a_{0})}f(a_{0})$
3. Вычисляем $f(x_{0})$
4. В качестве нового интервала выбираем ту половину отрезка, на концах которого функция имеет разные знаки: $[a_{0}, b_{0}]\text{ либо } [b_{0}, x_{0}]$ 
5. Вычисляем $x_{1}$ и так далее повторяем 2-4 шаги

##### Рабочая формула метода:
$$
x_{i}=a_{i}- \frac{b_{i}-a_{i}}{f(b_{i})-f(a_{i})}f(a_{i}) \text{ или } x_{i}= \frac{a_{i}f(b_{i})-b_{i}f(a_{i})}{f(b_{i})-f(a_{i})}
$$

##### Критерии окончания:
$$
|x_{n}-x_{n-1}|\leq \epsilon \text{ или } |a_{n}-b_{n}|\leq\epsilon \text{ или } |f(x_{n})| \leq \epsilon
$$

##### Приближенное значение корня:
$$x^*=x_{n}$$
### 11. Метод секущих.
Упрощение метода Ньютона
Заменяем $f'(x)$ разностным приближением:
$f'(x_{i})\approx \frac{{f(x_{i})-f(x_{i-1})}}{x_{i}-x_{i-1}}$

Рабочая формула метода:
$$
x_{i+1}=x_{i}- \frac{{x_{i}-x_{i-1}}}{f(x_{i})-f(x_{i-1})}f(x_{i});i=1, 2\dots
$$

Метод является **двух**шаговым, то есть новое приближение $x_{i+1}$ определяется **двумя** предыдущими итерациями ($x_{i}$ и $x_{i-1}$)
Выбор $x_{0}$ определяется как и в методе Ньютона, $x_1$ выбирается рядом с начальным самостоятельно (e.g. $x_{1}=x_{0}+\epsilon$)

Критерий окончания: $|x_{n}-x_{n-1}|\leq\epsilon$ или $|f(x_{n})|\leq \epsilon$
Приближенное значение корня: $x^*=x_n$

+: Меньший объем вычислений
-: Порядок сходимости ниже, чем у метода касательных (равен золотому сечению)
### 12. Методы решения системы нелинейных уравнений. Метод простой итерации.
#### Достаточное условие сходимости:
$$
\underset{[x \in G]}{max} |\phi'(x)|\leq q<1 \text{ или } \underset{[x \in G]}{max} \text{ } \underset{[i]}{max}\sum^n_{j=1}|\frac{\partial \phi_{i}(X)}{\partial x_{j}}|\leq q<1
$$
$$
\begin{align} \\

|\frac{\partial \phi_{1}}{\partial x_{1}}| + |\frac{\partial \phi_{1}}{\partial x_{2}}| + \dots + |\frac{\partial \phi_{1}}{\partial x_{n}}| < 1 \\
|\frac{\partial \phi_{2}}{\partial x_{1}}| + |\frac{\partial \phi_{2}}{\partial x_{2}}| + \dots + |\frac{\partial \phi_{2}}{\partial x_{n}}| < 1 \\
\dots \\
|\frac{\partial \phi_{n}}{\partial x_{1}}| + |\frac{\partial \phi_{n}}{\partial x_{2}}| + \dots + |\frac{\partial \phi_{n}}{\partial x_{n}}| < 1 \\

\end{align}
$$

#### Критерий окончания итерационного процесса
$\underset{1<i<n}{max}|x^{(k+1)}-x^k_{i}|\leq\epsilon$

#### Описание алгоритма на примере
$$
\begin{cases}
x_{1}=0.3-0.1x_{1}^2-0.2x_{2}^2 \\
x_{2}=0.7-0.2x_{1}^2-0.1x_{1}x_{2}
\end{cases}
$$
Выберем начальное приближение: $x_{1}^{(0)}=1 \quad x_{2}^{(0)}=1$

И дальше просто подставляем x_1 и x_2 в системку, получаем ответ, и снова подставляем

### 13. Методы решения системы нелинейных уравнений. Метод Ньютона.
%% 
Пусть для вычисления неизвестных $x_{1},x_{2},\dots,x_{n}$ требуется решить систему нелинейных уравнений:
$$
\begin{cases}
F_{1}(x_{1},x_{2},\dots,x_{n})=0 \\
F_{2}(x_{1}, x_{2}, \dots, x_{n})=0 \\
\dots \\
F_{n}(x_{1}, x_{2}, \dots, x_{n})=0
\end{cases}
$$
 %%
#### 1. Суть метода

Пусть у нас система из nn нелинейных уравнений:

$F(x) = 0, \quad F(x) = \begin{bmatrix} f_1(x_1, \dots, x_n) \\ f_2(x_1, \dots, x_n) \\ \vdots \\ f_n(x_1, \dots, x_n) \end{bmatrix}, \quad x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}.$
Метод Ньютона ищет решение итерациями:
$(x(k)),x^{(k+1)} = x^{(k)} - J_F(x^{(k)})^{-1} \, F(x^{(k)}),$где
- $F(x^{(k)})$ — вектор значений системы в точке,
- $J_F(x)$ — якобиан (матрица частных производных):
$$J_F(x) = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_n}{\partial x_n} \end{bmatrix}.
$$
То есть на каждом шаге решаем **линейную систему**: $J_F(x^{(k)}) \, \Delta x = -F(x^{(k)}),$
и обновляем: $x^{(k+1)} = x^{(k)} + \Delta x.$
#### 2. Пример
Рассмотрим систему из двух уравнений с двумя переменными:
$\begin{cases} f_1(x, y) = x^2 + y^2 - 4 = 0 \\ f_2(x, y) = e^x + y - 1 = 0 \end{cases}$
##### Шаг 1. Якобиан
Вычисляем частные производные:
$J_F(x,y) = \begin{bmatrix} \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\ \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} \end{bmatrix} = \begin{bmatrix} 2x & 2y \\ e^x & 1 \end{bmatrix}.$
##### Шаг 2. Выбираем начальное приближение
Возьмём $x^{(0)} = 1, \, y^{(0)} = 1.$
##### Шаг 3. Вычисляем $F(x^{(0)}, y^{(0)})$

$F(1,1) = \begin{bmatrix} 1^2 + 1^2 - 4 \\ e^1 + 1 - 1 \end{bmatrix} = \begin{bmatrix} -2 \\ e \end{bmatrix}.$
##### Шаг 4. Вычисляем якобиан в точке
$J_F(1,1) = \begin{bmatrix} 2 & 2 \\ e & 1 \end{bmatrix}.$
##### Шаг 5. Решаем систему
Нужно решить:
$J_F(1,1) \, \Delta x = -F(1,1).$
То есть:
$$\begin{bmatrix} 2 & 2 \\ e & 1 \end{bmatrix} \begin{bmatrix} \Delta x \\ \Delta y \end{bmatrix} = \begin{bmatrix} 2 \\ - e \end{bmatrix}.$$
Из первого уравнения: $2\Delta x + 2\Delta y = 2 \Rightarrow \Delta x + \Delta y = 1.$
Из второго: $\Delta x + \Delta y = -e.$
Подставляем $\Delta y = 1 - \Delta x$:
$$
\begin{align}
e\Delta x + (1 - \Delta x) = -e, \\
(e - 1)\Delta x = -e - 1,  \\
\Delta x = \frac{-e - 1}{e - 1}, \quad \Delta y = 1 - \Delta x.
\end{align}
$$
##### Шаг 6. Новое приближение
$x^{(1)} = 1 + \Delta x, \quad y^{(1)} = 1 + \Delta y$.
После этого повторяем процесс до сходимости.
#### 3. Идея в двух словах
- Берём точку → считаем систему и якобиан.
- Решаем линейную систему для приращения.
- Двигаемся к новой точке.
- Повторяем, пока не стабилизируется.
### 14. Численное интегрирование. Метод прямоугольников.
### 15. Численное интегрирование. Метод трапеций.
Подынтегральную функцию на каждом отрезке $[x_{i};x_{i+1}]$ заменяют интерполяционным многочленом первой степени: $f(x)\approx \phi_{i}(x)=a_{i}x+b$
Используют линейную интерполяцию, т.е. график функции $y=f(x)$ представляется в виде ломаной, соединяющей точки $(x_{i}, y_{i})$. Площадь всей фигуры (криволинейной трапеции):
$$
S_{общ} = S_{1}+S_{2}+\dots+S_{n}=\frac{{y_{0}+y_{1}}}{2}h_{1}+\frac{{y_{1}+y_{2}}}{2}h_{2}+\dots+\frac{{y_{n-1}+y_{n}}}{2}h_{n}
$$
$$
y_{0}=f(a);y_{n}=f(b); y_{i}=f(x_{i}); h_{i}=x_{i}-x_{i-1}
$$

Складывая все эти равенства, получаем формулу трапеций для численного интегрирования:
$$
\int^b_{a}f(x)dx=\frac{1}{2}\sum^n_{i=1}h_{i}(y_{i-1}+y_{i})
$$

При $h_{i}=h=\frac{b-a}{n}= const$ формула трапеций принимает два вида:
$$
\int^b_{a}f(x)dx=h*\left( \frac{y_{0}+y_{n}}{2} +\sum^{n-1}_{i=1}y_{i}\right) \text{ или } \int^b_{a}f(x)dx=\frac{h}{2}*\left( y_{0}+y_{n}+2\sum^{n-1}_{i=1}y_{i} \right)
$$

### 16. Численное интегрирование. Метод Симпсона.
%%TODO: тут бы теории еще написать, но для минималочки и формулы как будто хватит%%
Основные поинты:
- n должно быть нечетным, где n - это количество точек
- На каждом отрезке $[x_{0}, x_{2}],[x_{2},x_{4}]\dots,[x_{i-1},x_{i+1}],\dots,[x_{n-2}, x_{n}]$ подынтегральную функцию заменим интерполяционным многочленом второй степени: $f(x)\approx \phi_{i}(x)=a_{i}x^2+b_{i}x+c_{i}, \quad x_{i-1}\leq x\leq x_{i+1}$

На каждом отрезке функция заменяется параболой через три точки
$$
\int^b_{a}f(x)=\frac{h}{3}[(y_{0}+4(y_{1}+y_{3}+\dots+y_{n-1})+2(y_{2}+y_{4}+\dots+y_{n-2})+y_{n})]
$$
### 17. Правило Рунге.

Непосредственное использование оценки погрешности неудобно, т.к. требует вычисление производной функции $f(x)$, особенно для подынтегральных функций сложного вида. В вычислительной практике используется правило Рунге

Правило Рунге - это имперический способ оценки погрешности, основанный на сравнении результатов вычислений, проводимых с разными шагами $h$:
$$
I=I_{h/2}\approx \frac{I_{h/2}-I_{h}}{2^k-1}
$$
$I$ - точное значение интеграла
$I_{h/2}, I_h$ - приближенные значения интеграла, вычисленные с различными шагами h
$k$ - порядок точности квадратурной формулы ($k=2$ для формул средних прямоугольников и трапеций, $k=4$ - для формулы Симпсона)
### 18. Интерполяция функции.
Топ 4 самых популярных интерполяций:
- Линейная
	- Промежуточная точка (расположена между двумя узловыми точками $(x_{i}, y_{i})$ и $(x_{i+1}, y_{i+1})$), лежит на отрезке прямой, соединяющей две ближайшие узловые точки
	- Простейший и часто используемый вид локальной интерполяции
- Квадратичная
	- Промежуточная точка между двумя узловыми точками лежит на отрезке параболы, соединяющей эти две узловые точки
- Полиномиальная
	- Промежуточные точки вычисляются как значение некоторого многочлена $P_{n}(x)$, причем $P_{n}(x_{i})=f(x_{i})$
- Сплайновая
	- Промежуточные точки вычисляются с помощью отрезков полиномов невысокой степени, проходящих через узловые точки и поддерживающие определенные условия стыковки в концевых точках
### 19. Интерполяционная формула Лагранжа.
Формула многочлена Лагранжа:
$$
L_{n}(x)=\sum^n_{i=0}y_{i}l_{i}(x)
$$
$$
L_{n}(x)=\sum^n_{i=0}y_{i}  \frac{(x-x_{0})(x-x_{1})\dots(x-x_{i-1})(x-x_{i+1})\dots(x-x_{n})}{(x_i-x_{0})(x_{i}-x_{1})\dots(x_{i}-x_{i-1})(x_{i}-x_{i+1})\dots(x_{i}-x_{n})}
$$

+: Малая погрешность при n<20
-: С изменением числа узлов приходится проводить все вычисления заново

Линейная и квадратичная интерполяции являются частными случаями интерполяции многочленом Лагранжа
При $n=1$ (два узла и первая степень многочлена):
$L_{1}(x)=y_{0} \frac{x-x_{1}}{x_{1}-x_{0}}+y_{1} \frac{x-x_{0}}{x_{1}-x_{0}}$

При n=2 (три узла и вторая степень многочлена):
$L_2(x)=y_0\frac{(x-x_{1})(x-x_{2})}{(x_{0}-x_{1})(x_{0}-x_{2})}+y_{1}\frac{(x-x_{0})(x-x_{2})}{(x_{1}-x_{0})(x_{1}-x_{2})}+y_{2}\frac{(x-x_{0})(x_{2}-x_{0})}{(x-x_{1})(x_{2}-x_{1})}$
## TODO: перепроверить правильно ли переписал!!!!!
### 20. Интерполяционные формулы Ньютона с разделенными разностями.
###### 1. Задача
Как и в Лагранже, у нас есть набор точек $(x_0,y_0),\dots,(x_n,y_n)$.
Мы хотим построить многочлен $P_n(x)$, который через них проходит.
###### 2. Идея Ньютона
Вместо одной большой громоздкой формулы, как у Лагранжа, Ньютон строит многочлен **постепенно**, добавляя новые члены при добавлении новых узлов.
Форма:
$$
P_n(x) = a_0 + a_1(x-x_0) + a_2(x-x_0)(x-x_1) + \dots + a_n(x-x_0)(x-x_1)\cdots(x-x_{n-1}).
$$
###### 3. Коэффициенты $a_i$
Они вычисляются через **разделённые разности**.
Определение:
* Нулевая разность:
$$
[y_i] = f(x_i)=y_i.
$$
* Первая разность:
$$
[x_i, x_{i+1}] = \frac{y_{i+1}-y_i}{x_{i+1}-x_i}.
$$
* Вторая разность:
$$
[x_i,x_{i+1},x_{i+2}] = \frac{[x_{i+1},x_{i+2}] - [x_i,x_{i+1}]}{x_{i+2}-x_i}.
$$
* И так далее.
###### 4. Формула полностью
$$
P_n(x) = [x_0] + [x_0,x_1](x-x_0) + [x_0,x_1,x_2](x-x_0)(x-x_1) + \dots + [x_0,x_1,\dots,x_n](x-x_0)\cdots(x-x_{n-1}).
$$
###### 5. Преимущество перед Лагранжем
* Если добавляем новую точку, **не нужно пересчитывать всё заново**: просто считаем новую разделённую разность и добавляем ещё один член.
* Удобно оформляется таблицей (таблица разделённых разностей).
### 21. Интерполяционные формулы Ньютона с конечными разностями.
- Узлы равномерные: $x_i=x_0+ih$, шаг h постоянный.
- **Прямые (вперёд) конечные разности** по таблице $y_i=f(x_i)$:
    $\Delta y_i = y_{i+1}-y_i,\quad \Delta^2 y_i=\Delta(\Delta y_i),\ \dots$
    Обозначим $t=\dfrac{x-x_0}{h}$.
    $P_n(x)=y_0 + t\,\Delta y_0 + \frac{t(t-1)}{2!}\,\Delta^2 y_0 + \frac{t(t-1)(t-2)}{3!}\,\Delta^3 y_0+\cdots$
- **Обратные (назад) конечные разности** у конца таблицы:  
    $\nabla y_i = y_i-y_{i-1},\quad \nabla^2 y_i=\nabla(\nabla y_i),\ \dots$ 
    Обозначим $t=\dfrac{x-x_n}{h}$.
    $P_n(x)=y_n + t\,\nabla y_n + \frac{t(t+1)}{2!}\,\nabla^2 y_n + \frac{t(t+1)(t+2)}{3!}\,\nabla^3 y_n+\cdots$
- **Когда какую?** Ближе к началу таблицы — формула «вперёд», ближе к концу — «назад».
- **Интуиция по точности:** центрировать t поближе к нулю ⇒ слагаемые меньшие по модулю ⇒ стабильнее.
### 22. Интерполяционные формулы Гаусса, Стирлинга, Бесселя.
#### Гаусса
##### Первая (x>a)
$$
\begin{align}
P_{n}(x)=y_{0}+t\Delta y_{0} + \\
\frac{t(t-1)}{2!}\Delta^2y_{-1} +\\
\frac{(t+1)t(t-1)}{3!}\Delta^3y_{-1} +  \\
\dots+ \\

\frac{(t+n-1)\dots(t-n+1)}{(2n-1)!}\Delta^{2n-1}y_{-(n-1)} + \\
\frac{(t+n-1)\dots(t-n)}{(2n)!}\Delta^{2n}y_{-(n)}
\end{align}
$$
##### Вторая (x<a)
$$
\begin{align}
P_{n}(x)=y_{0}+t\Delta y_{-1}+\frac{t(t+1)}{2!}\Delta^2y_{-1}+ \\
\frac{(t+1)t(t-1)}{3!}\Delta^3y_{-2}+ \\
\frac{(t+2)(t+1)t(t-1)}{4!}\Delta^4y_{-2} + \\
\dots + \\
\frac{(t+n-1)\dots(t-n+1)}{(2n-1)!}\Delta^{2n-1}y_{-n}+ \\
\frac{(t+n)(t+n-1)\dots(t-n+1)}{(2n)!}\Delta^{2n}y_{-n}+
\end{align}
$$
#### Стирлинга
Арифметическое среднее первой и второй интерполяционных формул Гаусса

#### Бесселя
- Для интерполирования при значениях t, близких к 0.5
- 
### 23. Интерполяция сплайнами.
##### Определение
Сплайном степени $n$ называется функция $S_n(x)$ обладающая следующими свойствами:
- Функция непрерывна на отрезке $[x_0;x_n]$ вместе со своими производными
- На каждом частичном отрезке $[x_{i-1};x_i]$ функция $S_{n,i}(x)$ является многочленом $P_{n, i}(x)$ степени n
##### Характеристики
- Степень сплайна - максимальная из степеней использованных полиномов
- Гладкость - максимальный порядок непрерывной производной
- Дефект - разность между степенью 
### 24. Аппроксимация функции.
Основная задача аппроксимации - построение эмпирической формулы, для которой $f(x_{i})\approx \phi(x_{i})$
### 25. Метод наименьших квадратов.
### 26. Линейная, квадратичная аппроксимация.
### 27. Аппроксимация степенной, экспоненциальной и логарифмической функций.
### 28. Решения задачи Коши. Метод Эйлера.
### 29. Решения задачи Коши. Методы Рунге-Кутта.
### 30. Многошаговые методы. Метод Адамса.
### 31. Многошаговые методы. Метод Милна.

##