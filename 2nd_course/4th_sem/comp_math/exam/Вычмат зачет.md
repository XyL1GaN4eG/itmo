---
Theme: "[[00 ВычМат]]"
tags:
  - вычмат
type: note
creation date: 2025-06-20 15:59
modification date: Friday 20th June 2025 15:59:43
---




Notes:

## 1. Основные свойства численных методов

1. **Приближённость** — численные методы не дают точного решения, а лишь его приближение.
2. **Сходимость** — метод считается сходящимся, если при уменьшении шага вычислений приближённое решение стремится к точному.
3. **Устойчивость** — метод устойчив, если малые изменения входных данных вызывают только малые изменения результата.
4. **Погрешность**:
   - **Абсолютная**: $|\tilde{x} - x|$
   - **Относительная**: $\frac{|\tilde{x} - x|}{|x|}$
   - **Оценка погрешности** используется для контроля точности.
5. **Эффективность** — соотношение между точностью метода и вычислительными затратами (время, память).
6. **Универсальность** — возможность применять метод к широкому классу задач.

---

## 2. Решение систем линейных уравнений. Метод Гаусса

**Цель:** найти решение системы $Ax = b$.

### Этапы метода:
1. **Прямой ход** — приведение матрицы $A$ к верхнетреугольному виду методом исключения Гаусса:
   - Устраняем переменные по очереди из нижележащих уравнений.
2. **Обратный ход** — подстановка найденных значений снизу вверх.

Пример:
```
Исходная система:
a₁₁x₁ + a₁₂x₂ + a₁₃x₃ = b₁  
a₂₁x₁ + a₂₂x₂ + a₂₃x₃ = b₂  
a₃₁x₁ + a₃₂x₂ + a₃₃x₃ = b₃  

После прямого хода:
x₃ = ...  
x₂ = ...  
x₁ = ...
```

---

## 3. Метод Гаусса с выбором главного элемента

**Проблема обычного метода:** деление на элемент, близкий к нулю → большая погрешность.

### Улучшение:
- На каждом шаге выбирается максимальный по модулю элемент в текущем столбце (опорный элемент).
- Меняются местами строки (иногда и столбцы) так, чтобы опорный элемент оказался на диагонали.

### Преимущества:
- Повышается численная устойчивость.
- Снижается влияние ошибок округления.

---

## 4. Метод Гаусса-Зейделя

**Итерационный метод** для решения СЛАУ $Ax = b$.

### Формула итераций:
$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j<i} a_{ij}x_j^{(k+1)} - \sum_{j>i} a_{ij}x_j^{(k)} \right)
$$

### Идея:
- Используем уже вычисленные значения на текущей итерации.

### Условия сходимости:
- Диагональное преобладание: $|a_{ii}| > \sum_{j \ne i} |a_{ij}|$
- Симметричность и положительная определённость матрицы $A$

### Преимущества:
- Простота реализации
- Быстрая сходимость при выполнении условий

---

## 5. Метод простой итерации (Якоби)

Итерационная схема:
$$
x^{(k+1)} = Bx^{(k)} + c
$$
Преобразуется система $Ax = b$ к виду $x = Bx + c$.
### Условие сходимости:
- Норма матрицы $B$: $\|B\| < 1$ (обычно — по максимуму строк или спектральный радиус)

### Итерационный шаг:
Каждая переменная считается на основе значений с предыдущего шага:
$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \ne i} a_{ij} x_j^{(k)} \right)
$

Метод Якоби медленнее, чем метод Гаусса-Зейделя.

---

## 6. Условия сходимости итерационных методов решения СЛАУ

### Для метода простой итерации:
- Матрица $A$ должна быть приведена к виду $x = Bx + c$
- **Необходимое и достаточное условие:** спектральный радиус матрицы $B$ меньше 1:
  $
  \rho(B) = \max |\lambda_i| < 1
  $

### Достаточные условия:
- Диагональное преобладание:
$
  |a_{ii}| > \sum_{j \ne i} |a_{ij}|, \quad \forall i
$
- Симметричность и положительная определённость $A$ (для Гаусса-Зейделя и сопряжённых градиентов)

---

## 7. Метод касательных (Ньютона)

Решает уравнение $f(x) = 0$.

### Формула:
$
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
$

### Условия сходимости:
- $f(x)$ и $f'(x)$ — непрерывны
- $f'(x) \ne 0$ в окрестности корня
- Хорошее начальное приближение

### Преимущества:
- Быстрая квадратичная сходимость при хорошей начальной точке

### Недостатки:
- Требуется вычислять $f'$
- Неустойчив при плохом начальном приближении

---

## 8. Метод деления отрезка пополам (бисекции)

Простейший метод решения $f(x) = 0$, если $f(a) \cdot f(b) < 0$.

### Шаг:
$
c = \frac{a + b}{2}
$
- Если $f(a) \cdot f(c) < 0$, то $b = c$
- Иначе $a = c$

### Остановка:
$
|b - a| < \varepsilon \quad \text{или} \quad |f(c)| < \varepsilon
$
### Преимущества:
- Гарантированная сходимость

### Недостатки:
- Медленная сходимость (линейная)

---

## 9. Метод простой итерации (для уравнений)

Уравнение $f(x) = 0$ преобразуется к виду:
$
x = \varphi(x)
$
Итерации:
$
x_{n+1} = \varphi(x_n)
$

### Условие сходимости:
- В окрестности корня $|\varphi'(x)| < 1$

### Замечания:
- Метод прост, но выбор $\varphi(x)$ — не всегда очевиден
- Сходимость — линейная

---

## 10. Метод хорд

Решение уравнения $f(x) = 0$ с помощью двух точек $x_0$, $x_1$.

### Формула:
$
x_{n+1} = x_n - f(x_n) \cdot \frac{x_n - x_0}{f(x_n) - f(x_0)}
$
- В отличие от секущей, $x_0$ фиксирована.

### Преимущества:
- Не требует производной
- Стабильнее Ньютона

### Недостатки:
- Медленнее Ньютона
- Сходимость — линейная или суперлинейная


## 11. Метод секущих

Метод решения уравнения $f(x) = 0$, похожий на метод хорд, но использует две переменные точки:  
$
x_{n+1} = x_n - f(x_n) \cdot \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}
$

### Характеристики:
- Не требует производной
- Использует последние два приближения
- Сходимость сверхлинейная (порядка $\approx 1.618$)

---

## 12. Методы решения системы нелинейных уравнений. Метод простой итерации

Для системы $F(x) = 0$, переписываем:
$
x = \varphi(x)
$

### Алгоритм:
1. Задать начальное приближение $x^{(0)}$
2. Выполнять итерации:
   $
   x^{(k+1)} = \varphi(x^{(k)})
   $
3. Проверять условие остановки: $\|x^{(k+1)} - x^{(k)}\| < \varepsilon$

### Условие сходимости:
- Норма матрицы Якоби $\|\varphi'(x)\| < 1$ в области сходимости

---

## 13. Метод Ньютона для системы нелинейных уравнений

Пусть $F(x) = 0$, где $x \in \mathbb{R}^n$, $F: \mathbb{R}^n \to \mathbb{R}^n$

### Алгоритм:
1. На шаге $k$ вычисляем Якобиан $J(x^{(k)})$
2. Решаем СЛАУ:
   $
   J(x^{(k)}) \cdot \Delta x^{(k)} = -F(x^{(k)})
   $
3. Обновление:
   $
   x^{(k+1)} = x^{(k)} + \Delta x^{(k)}
   $

### Сходимость:
- Быстрая (квадратичная), если начальное приближение близко к решению

---

## 14. Численное интегрирование. Метод прямоугольников

Приближённое вычисление определённого интеграла:
$
\int_a^b f(x) dx
$

### Формула (по левым прямоугольникам):
$
I \approx h \sum_{i=0}^{n-1} f(x_i), \quad h = \frac{b - a}{n}
$

Варианты:
- Левые прямоугольники
- Правые прямоугольники
- Средние прямоугольники

### Погрешность:
- $O(h)$ (линейный порядок)

---

## 15. Численное интегрирование. Метод трапеций

Формула:
$
I \approx \frac{h}{2} \left(f(x_0) + 2f(x_1) + \dots + 2f(x_{n-1}) + f(x_n)\right)
$

или
$
I \approx \frac{h}{2} \sum_{i=1}^{n} (f(x_{i-1}) + f(x_i))
$

### Погрешность:
- $O(h^2)$ — квадратичная сходимость

---

## 16. Численное интегрирование. Метод Симпсона (парабол)

Требование: $n$ — чётное количество отрезков.

### Формула:
$
I \approx \frac{h}{3} \left(f(x_0) + 4f(x_1) + 2f(x_2) + \dots + 4f(x_{n-1}) + f(x_n)\right)
$

### Погрешность:
- $O(h^4)$ — высокая точность

---

## 17. Правило Рунге

Используется для оценки точности численного решения.

Пусть $y_h$ — решение с шагом $h$, $y_{h/2}$ — с шагом $h/2$, $p$ — порядок метода.

### Формула оценки погрешности:
$
R = \frac{y_{h/2} - y_h}{2^p - 1}
$

Если $R < \varepsilon$, результат считается достаточно точным.

---

## 18. Интерполяция функции

### Задача:
Построить функцию $P(x)$, проходящую через заданные точки $(x_i, y_i)$

Применяется, когда известны значения функции в точках, а нужно вычислить её в других.

Методы:
- Лагранжа
- Ньютона
- Сплайны

---

## 19. Интерполяционная формула Лагранжа

Многочлен $n$-й степени:
$
L(x) = \sum_{i=0}^{n} y_i \cdot l_i(x)
$
где
$
l_i(x) = \prod_{\substack{j=0 \\ j \ne i}}^{n} \frac{x - x_j}{x_i - x_j}
$

### Преимущества:
- Не требует перестроения при добавлении точек

### Недостатки:
- Высокая степень многочлена → осцилляции
- Большая вычислительная сложность

---

## 20. Интерполяционные формулы Ньютона с разделенными разностями

Используются при неравномерной сетке узлов.

### Многочлен Ньютона:
$
P(x) = f[x_0] + f[x_0,x_1](x - x_0) + f[x_0,x_1,x_2](x - x_0)(x - x_1) + \dots
$

### Разделённые разности:
$
f[x_i] = y_i, \quad f[x_i,x_{i+1}] = \frac{f[x_{i+1}] - f[x_i]}{x_{i+1} - x_i}
$
$
f[x_i,x_{i+1},x_{i+2}] = \frac{f[x_{i+1},x_{i+2}] - f[x_i,x_{i+1}]}{x_{i+2} - x_i}, \quad \text{и т.д.}
$

### Преимущества:
- Удобно дополнять новыми точками
- Подходит для неравномерных узлов
